{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "models = [\"llama-2-13b-chat-hf\", \"llama-2-7b-chat-hf\", \"llama-2-70b-chat-hf\", \"chatgpt\"]\n",
    "for model in models:\n",
    "    data_path = f\"./task/prompt_task/prompt_judge/origin/{model}/\"\n",
    "    save_path = f\"./task/prompt_task/prompt_response/origin/{model}/\"\n",
    "    for path in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data = [\n",
    "            {\n",
    "                \"id\": i[\"id\"],\n",
    "                \"user_query\": i[\"user_query\"],\n",
    "                f\"{model}_response\": i[f\"{model}_response\"],\n",
    "            }\n",
    "            for i in data\n",
    "        ]\n",
    "        with open(os.path.join(save_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46594081233963097\n",
      "0.7714384144358594\n",
      "0.8484265746593843\n",
      "0.6531434659221789\n",
      "0.3597286898454558\n",
      "0.6006341297980338\n",
      "0.4867379039580224\n",
      "0.542081735728645\n",
      "0.26703220434065644\n",
      "0.46346787468538453\n",
      "0.5710491377104308\n",
      "0.5832535232802836\n",
      "0.33852176224895136\n",
      "0.5171057479415075\n",
      "0.2097951160862309\n",
      "0.4926405971447758\n",
      "0.3526569177518016\n",
      "0.4357770594881209\n",
      "0.05931008145087083\n",
      "0.8043042165494724\n",
      "0.27147335498457265\n",
      "0.3029690674976246\n",
      "0.48809127069306096\n",
      "0.4050042246894413\n",
      "0.6103677439216151\n",
      "0.9474597430224703\n",
      "0.7342951408723789\n",
      "0.7006984792084321\n",
      "0.33231093897007435\n",
      "0.5659576919451873\n",
      "1.0\n",
      "0.5617817752909144\n",
      "0.538516402482136\n",
      "0.721303048708511\n",
      "0.5526573485331544\n",
      "0.8890211381545633\n",
      "0.6787517690921893\n",
      "0.8499004849916231\n",
      "0.4497151410673437\n",
      "0.5899786561309343\n",
      "0.45059826416550297\n",
      "0.5382103264168185\n",
      "0.6967745114517596\n",
      "0.7399330013095038\n",
      "0.6129969608930202\n",
      "0.5251861772557642\n",
      "0.612978422728667\n",
      "0.483637593499632\n",
      "0.1937522404146077\n",
      "0.571488270370899\n",
      "0.7040353011109265\n",
      "0.5880539455274474\n",
      "0.717648679394393\n",
      "0.6787595910387618\n",
      "0.7567161637167291\n",
      "0.49117957231859505\n",
      "0.681138008520892\n",
      "0.7452084748331046\n",
      "0.5815118444477166\n",
      "0.4817891889845976\n",
      "0.683418640219125\n",
      "0.5897884372189665\n",
      "0.4738283920442093\n",
      "0.4389052414897777\n",
      "0.6926131117976821\n",
      "0.48877673742941274\n",
      "0.6181888295493145\n",
      "0.21461979269255585\n",
      "0.3472736685771921\n",
      "0.66878283459943\n",
      "0.49984447746199145\n",
      "0.5884182711168757\n",
      "0.5910675853921238\n",
      "0.3890280661568605\n",
      "0.5590492794878462\n",
      "0.5780477734965901\n",
      "0.8115248499611655\n",
      "0.5494933169039156\n",
      "0.4953665692033657\n",
      "0.6793602538349273\n",
      "0.7184289493664213\n",
      "0.6808365132055919\n",
      "0.2547170193408869\n",
      "0.7552384751842732\n",
      "0.5778162607436603\n",
      "0.324460652376788\n",
      "0.6678910257787014\n",
      "0.5992964304118655\n",
      "0.6294852691309338\n",
      "0.6841286989903493\n",
      "0.6683328701838605\n",
      "0.5711491070256647\n",
      "0.5213070608716154\n",
      "0.6291227173393948\n",
      "0.6607902660124603\n",
      "0.3482848142452179\n",
      "0.42512891012469745\n",
      "0.6356815983880659\n",
      "0.5734866704434283\n",
      "0.6464904592426272\n",
      "0.6318001978616586\n",
      "0.7263710347758042\n",
      "0.5254719268956116\n",
      "0.6884680206353774\n",
      "0.6793412665591894\n",
      "0.31244915604038803\n",
      "0.7435971181881638\n",
      "0.6330573470136305\n",
      "0.5767968750544376\n",
      "0.698749222298911\n",
      "0.6004738701686643\n",
      "0.7520509403990919\n",
      "0.6636397963360618\n",
      "0.654416200734657\n",
      "0.5674006068554689\n",
      "0.5710681900365732\n",
      "0.5978565150434293\n",
      "0.6512855281310193\n",
      "0.746198885997899\n",
      "0.6524850098079894\n",
      "0.765104883579297\n",
      "0.7014391802783506\n",
      "0.6497027220585284\n",
      "0.7445267397012686\n",
      "0.49147802827799825\n",
      "0.5969124181928769\n",
      "0.8170861459287555\n",
      "0.6028363878545443\n",
      "0.6957493644811091\n",
      "0.6457453890148984\n",
      "0.7512549641582627\n",
      "0.637628794116705\n",
      "0.6750973534553506\n",
      "0.5339467984465911\n",
      "0.5987118638490708\n",
      "0.7357096530075652\n",
      "0.5047119661606171\n",
      "0.7403635425099238\n",
      "0.7437210585710837\n",
      "0.7034837085070356\n",
      "0.7167068442034072\n",
      "0.33255922853396164\n",
      "0.641328359156306\n",
      "0.7373962811943828\n",
      "0.7664336676736889\n",
      "0.6164988133041588\n",
      "0.8153892242296072\n",
      "0.732372371874896\n",
      "0.6514281975927065\n",
      "0.4096200779649677\n",
      "0.668182433695021\n",
      "0.5779842007985384\n",
      "0.6938337667955194\n",
      "0.6321089341501139\n",
      "0.7126655236480381\n",
      "0.6127279149737652\n",
      "0.5209106568938612\n",
      "0.6295751649783812\n",
      "0.5734054848202519\n",
      "0.6573046522205349\n",
      "0.8127925618814075\n",
      "0.576987442359866\n",
      "0.6831112429670017\n",
      "0.7743431891372878\n",
      "0.5656377806322304\n",
      "0.7950724114076148\n",
      "0.7327118717230291\n",
      "0.6718877559426301\n",
      "0.5228312459607801\n",
      "0.7344206368180158\n",
      "0.6546493552123883\n",
      "0.6212996037489379\n",
      "0.6465343532994545\n",
      "0.6890247287847787\n",
      "0.657065228725449\n",
      "0.679444298087731\n",
      "0.7994132807503508\n",
      "0.6437044784997471\n",
      "0.7206053794238825\n",
      "0.7320738911074491\n",
      "0.6482929176435043\n",
      "0.3936880408237314\n",
      "1.0\n",
      "0.7584238832806287\n",
      "0.5499115312700009\n",
      "0.7274730036931809\n",
      "0.6759621662148891\n",
      "0.6626344766188579\n",
      "0.702391015609074\n",
      "0.6533134710396405\n",
      "0.6864356662617602\n",
      "0.7303625773839414\n",
      "0.4857945691702842\n",
      "0.6376068813088392\n",
      "0.6420174144278489\n",
      "0.7450558737342344\n",
      "0.5808478638497546\n",
      "0.6405161342450693\n",
      "0.6884704445541273\n",
      "0.5362673755090178\n",
      "0.7160689884633893\n",
      "0.6597880224411345\n",
      "0.6941943014946772\n",
      "0.6897979573677776\n",
      "0.527849651198196\n",
      "0.7052417014077853\n",
      "0.674762745390366\n",
      "0.6261113270730819\n",
      "0.744278666251414\n",
      "0.6457499137254223\n",
      "0.6106771776608063\n",
      "0.7790935491295929\n",
      "0.6843749389375448\n",
      "0.7435557591770554\n",
      "0.7980186486443704\n",
      "0.7467705154780796\n",
      "0.7968689631263159\n",
      "0.7301505670711506\n",
      "0.6849440297312048\n",
      "0.7349582046692927\n",
      "0.6358611197322156\n",
      "0.5001310237142228\n",
      "0.721431372698464\n",
      "0.6602626711777737\n",
      "0.6356749104618914\n",
      "0.7535705693591691\n",
      "0.7354531497507047\n",
      "0.743829597139339\n",
      "0.5307721666078695\n",
      "0.6426912530579709\n",
      "0.7358373156304572\n",
      "0.7226360229187692\n",
      "0.6818623284749317\n",
      "0.7276997635284482\n",
      "0.6319383921849117\n",
      "0.8127885416583931\n",
      "0.7198670288356376\n",
      "0.8506881913719869\n",
      "0.799619405338591\n",
      "0.6900176343305026\n",
      "0.6398178739685842\n",
      "0.6925777548328678\n",
      "0.7087192764974216\n",
      "0.641757931319258\n",
      "0.7261651516006101\n",
      "0.728219548834091\n",
      "0.6142256603050962\n",
      "0.6613816506416998\n",
      "0.7129240948971647\n",
      "0.6657820092750014\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = \"./task/prompt_task/prompt_judge/origin/llama-2-7b-chat-hf/\"\n",
    "add_dir = \"./task/advanced_decoding/advanced_decoding_judge/gns_response/\"\n",
    "for file in os.listdir(data_dir):\n",
    "    add_path = os.path.join(add_dir, file)\n",
    "    with open(add_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        add = json.load(f)\n",
    "    data_path = os.path.join(data_dir, file)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    add = [i[\"llama-2-7b-chat-hf_response\"] for i in add]\n",
    "    data = [i[\"llama-2-7b-chat-hf_response\"] for i in data]\n",
    "    for i, j in zip(data, add):\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        print(sentence_bleu([i], j))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calculate_entropy(\n",
    "    probabilities,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a probability distribution.\n",
    "    \"\"\"\n",
    "    non_zero_probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -torch.sum(\n",
    "        non_zero_probabilities * torch.log(non_zero_probabilities)\n",
    "    )\n",
    "    return entropy.item()\n",
    "\n",
    "a = torch.rand(30000)\n",
    "a = nn.functional.softmax(a, dim=-1)\n",
    "a[a > 0.5]\n",
    "# entropy = torch.distributions.Categorical(a).entropy()\n",
    "# entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[0.0900, 0.2447, 0.6652]])\n",
      "tensor([0.8324])\n",
      "0.8323955535888672\n",
      "tensor(10.3090)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "input_ids = torch.tensor([[1., 2., 3.]])\n",
    "\n",
    "print(torch.tensor([2]).item())\n",
    "\n",
    "def calculate_entropy(\n",
    "    probabilities,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a probability distribution.\n",
    "    \"\"\"\n",
    "    non_zero_probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -torch.sum(\n",
    "        non_zero_probabilities * torch.log(non_zero_probabilities)\n",
    "    )\n",
    "    return entropy.item()\n",
    "\n",
    "\n",
    "# input_ids = input_ids.repeat_interleave(1, dim=0)\n",
    "# input_ids\n",
    "\n",
    "a = nn.functional.softmax(input_ids, dim=-1)\n",
    "print(a)\n",
    "print(torch.distributions.Categorical(a).entropy())\n",
    "print(calculate_entropy(a))\n",
    "print(torch.log(torch.tensor(30000.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"./wiki_entity_frequency.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [line.strip().split(\"\\t\") for line in data]\n",
    "    for i in data:\n",
    "        assert len(i) == 2\n",
    "    data = [(i[0], int(i[1])) for i in data if int(i[1]) >= 5]\n",
    "    data = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "    data = data[:5000]\n",
    "    # 分为10组，每组500个，每组抽样100个\n",
    "    import random\n",
    "    random.seed(45)\n",
    "    \n",
    "    data = [data[i : i + 500] for i in range(0, len(data), 500)]\n",
    "    data = [random.sample(i, 100) for i in data]\n",
    "    data = [sorted(i, key=lambda x: x[1], reverse=True) for i in data]\n",
    "    data = [j for i in data for j in i]\n",
    "    print(len(data))\n",
    "\n",
    "    # Constructing the query template that lets ChatGPT introduce entities\n",
    "    templates = [\n",
    "        \"Please introduce {}.\",\n",
    "        \"Please offer an introduction to {}.\",\n",
    "        \"Please provide an introduction of {}.\",\n",
    "        \"Please elaborate on {} and provide an introduction.\",\n",
    "        \"Please provide an introduction of {}.\",\n",
    "    ]\n",
    "    # save_data = [\n",
    "    #     {\n",
    "    #         \"id\": i,\n",
    "    #         \"user_query\": random.choice(templates).format(data[i][0]),\n",
    "    #     }\n",
    "    #     for i in range(len(data))\n",
    "    # ]\n",
    "    # with open(\"./Wiki_Entity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"./wiki_entity_frequency_top_fre.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in data:\n",
    "            # f.write(i[0] + \"\\t\" + str(i[1]) + \"\\n\")\n",
    "            f.write(str(i[1]) + \"\\n\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./wiki_entity_judge/llama-2-7b-chat-hf/Wiki_Entity.json') as f:\n",
    "    data = json.load(f)\n",
    "# divide into 10 parts\n",
    "data = [data[i:i + 100] for i in range(0, len(data), 100)]\n",
    "for i in range(10):\n",
    "    with open('./wiki_entity_judge/llama-2-7b-chat-hf/Wiki_Entity_{}.json'.format(i + 1), 'w') as f:\n",
    "        json.dump(data[i], f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"./task/RLHF/save_data/vicuna-7b/\"\n",
    "save_path = \"./task/RLHF/save_data_pure/vicuna-7b/\"\n",
    "for path in os.listdir(data_path):\n",
    "    with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i[\"id\"],\n",
    "            \"user_query\": i[\"user_query\"],\n",
    "            \"original_response\": i[\"original_response\"],\n",
    "            \"corrected_response\": i[\"corrected_response\"],\n",
    "        }\n",
    "        for i in data if i[\"round\"] == -1\n",
    "    ]\n",
    "    with open(os.path.join(save_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wiki_entity_frequency.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [line.strip().split(\"\\t\") for line in data]\n",
    "    for i in data:\n",
    "        assert len(i) == 2\n",
    "    data = [(i[0], int(i[1])) for i in data if int(i[1]) >= 5]\n",
    "    data = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "    data = data[:5000]\n",
    "    # 分为10组，每组500个，每组抽样100个\n",
    "    import random\n",
    "    random.seed(45)\n",
    "    \n",
    "    data = [data[i : i + 500] for i in range(0, len(data), 500)]\n",
    "    data = [random.sample(i, 100) for i in data]\n",
    "    data = [sorted(i, key=lambda x: x[1], reverse=True) for i in data]\n",
    "    data = [j for i in data for j in i]\n",
    "    print(len(data))\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Constructing the query template that lets ChatGPT introduce entities\n",
    "    templates = [\n",
    "        \"Please introduce {}.\",\n",
    "        \"Please offer an introduction to {}.\",\n",
    "        \"Please provide an introduction of {}.\",\n",
    "        \"Please elaborate on {} and provide an introduction.\",\n",
    "        \"Please provide an introduction of {}.\",\n",
    "    ]\n",
    "    save_data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": random.choice(templates).format(data[i][0]),\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    with open(\"./Wiki_Entity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"./wiki_entity_frequency_top.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in data:\n",
    "            f.write(i[0] + \"\\t\" + str(i[1]) + \"\\n\")\n",
    "    # 输出word id为横坐标，frequency为纵坐标的图像，并保存\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    x = np.arange(len(data))\n",
    "    y = [i[1] for i in data]\n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(\"wiki_entity_frequency_top.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"./task/RLHF/filtered_data/vicuna-7b/\"\n",
    "for path in os.listdir(data_path):\n",
    "    if path in [\n",
    "        # \"Bio-Medical.json\",\n",
    "        # \"Finance.json\",\n",
    "        # \"Science.json\",\n",
    "        # \"Education.json\",\n",
    "        # \"Open-Domain.json\",\n",
    "    ]:\n",
    "        continue\n",
    "    with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i[\"id\"],\n",
    "            \"user_query\": i[\"user_query\"],\n",
    "            \"original_response\": i[\"original_response\"],\n",
    "            \"corrected_response\": i[\"corrected_response\"],\n",
    "            \"hallucination\": i[\"hallucination\"],\n",
    "            \"round\": 0,\n",
    "        }\n",
    "        for i in data\n",
    "    ]\n",
    "\n",
    "    with open(os.path.join(data_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from response import check_exist\n",
    "\n",
    "path_dir = \"./ir/ir_response/response-10\"\n",
    "save_dir = \"./ir/ir_response_pure/response-10\"\n",
    "for model in os.listdir(path_dir):\n",
    "    print(\"Current model: \", model)\n",
    "    if model.startswith(\"claude\"):\n",
    "        key_ = \"response\"\n",
    "    else:\n",
    "        key_ = model + \"_response\"\n",
    "    for path in os.listdir(os.path.join(path_dir, model)):\n",
    "        data_path = os.path.join(path_dir, model, path)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data = [\n",
    "            {\n",
    "                \"id\": i[\"id\"],\n",
    "                \"user_query\": i[\"user_query\"].strip(),\n",
    "                model + \"_response\": i[key_].strip(),\n",
    "            }\n",
    "            for i in data\n",
    "        ]\n",
    "        check_exist(os.path.join(save_dir, model))\n",
    "        save_path = os.path.join(save_dir, model, path)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(data, g, indent=2, ensure_ascii=False)\n",
    "        print(\"Save to: \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"./70b/\"\n",
    "for path in os.listdir(data_path):\n",
    "    if path in [\n",
    "        # \"Bio-Medical.json\",\n",
    "        # \"Finance.json\",\n",
    "        # \"Science.json\",\n",
    "        # \"Education.json\",\n",
    "        # \"Open-Domain.json\",\n",
    "    ]:\n",
    "        continue\n",
    "    with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        # data = json.load(f)\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i[\"id\"],\n",
    "            \"user_query\": i[\"user_query\"].strip(),\n",
    "            \"reflexion_prompt\": i[\"reflexion\"],\n",
    "            \"llama-2-70b-chat-hf_response\": i[\"after_reflexion\"],\n",
    "        }\n",
    "        for i in data\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(data_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./results/self_reflexion/7b/\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f.readlines()][:20]\n",
    "    print(len(data))\n",
    "    # json file\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": data[i][\"question\"].strip(),\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "with open(\"./Open-Domain_demo.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"./task/main/judge/vicuna-13b\"\n",
    "add_dir = \"./annotation/query\"\n",
    "save_dir = \"./origin/vicuna-13b\"\n",
    "for file in os.listdir(add_dir):\n",
    "    add_path = os.path.join(add_dir, file)\n",
    "    with open(add_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        add = json.load(f)\n",
    "    data_path = os.path.join(data_dir, file)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    ids = [i[\"id\"] for i in add]\n",
    "    data = [i for i in data if i[\"id\"] in ids]\n",
    "    data = sorted(data, key=lambda x: x[\"id\"])\n",
    "    save_path = os.path.join(save_dir, file)\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "ModelList=(\"baichuan2-7b-intermediate-00220\" \"baichuan2-7b-intermediate-00440\" \"baichuan2-7b-intermediate-00660\" \"baichuan2-7b-intermediate-00880\" \"baichuan2-7b-intermediate-01100\" \"baichuan2-7b-intermediate-01320\" \"baichuan2-7b-intermediate-01540\" \"baichuan2-7b-intermediate-01760\" \"baichuan2-7b-intermediate-01980\" \"baichuan2-7b-intermediate-02200\" \"baichuan2-7b-intermediate-02420\")\n",
      "for model in $ModelList[*]; do\n",
      "    nohup python -u fact.py --all-files --model $model >> ./log/fact_$model.log 2>&1 &\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "a = list(os.listdir(\"./Baichuan2/\"))\n",
    "a = sorted(a, key=lambda x: x.split('-')[-1])\n",
    "a = \" \".join([f'\"{i}\"' for i in a])\n",
    "\n",
    "print(f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "ModelList=({a})\n",
    "for model in $ModelList[*]; do\n",
    "    nohup python -u fact.py --all-files --model $model >> ./log/fact_$model.log 2>&1 &\n",
    "done\n",
    "          \"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model:  llama-2-13b-chat-hf\n",
      "Current model:  llama-2-7b-chat-hf\n",
      "Current model:  alpaca-7b\n",
      "Current model:  vicuna-13b\n",
      "Current model:  text-davinci-003\n",
      "Current model:  vicuna-7b\n",
      "Current model:  chatgpt\n",
      "Current model:  text-davinci-002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_dir = \"./judge/\"\n",
    "for model in os.listdir(path_dir):\n",
    "    print(\"Current model: \", model)\n",
    "    for path in os.listdir(os.path.join(path_dir, model)):\n",
    "        data_path = os.path.join(path_dir, model, path)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data = [d for d in data if model + \"_judge_raw\" not in d.keys() or (model + \"_judge_raw\" in d.keys() and d[model + \"_judge_raw\"] != \"TIMEOUT\" and d[model + \"_judge_raw\"] != \"FAILED\")]\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_dir = \"./fact_judge/fact/\"\n",
    "delete_dir = \"./fact_judge/judge/\"\n",
    "for model in os.listdir(path_dir):\n",
    "    print(\"Current model: \", model)\n",
    "    for path in os.listdir(os.path.join(path_dir, model)):\n",
    "        data_path = os.path.join(path_dir, model, path)\n",
    "        delete_path = os.path.join(delete_dir, model, path)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # del_lst = []\n",
    "        for i in tqdm(range(len(data)), ncols=100):\n",
    "            # if model + \"_judge_raw\" in data[i].keys():\n",
    "            #     try:\n",
    "            #         assert len(data[i][model + \"_judge\"]) == len(data[i][model + \"_fact\"])\n",
    "            #     except:\n",
    "            #         print(data_path + \"    \" + str(data[i][\"id\"]) + \"    \" + str(data[i][model + \"_judge_raw\"]))\n",
    "            if isinstance(data[i][model + \"_fact\"], str):\n",
    "                print(data_path + \"\\t\" + str(data[i][\"id\"]) + \"\\t\" + str(data[i][model + \"_fact\"]))\n",
    "                data[i][model + \"_fact\"] = [data[i][model + \"_fact\"].strip()]\n",
    "                # del_lst.append(i)\n",
    "                try:\n",
    "                    with open(delete_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        delete_data = json.load(f)\n",
    "                    id = data[i][\"id\"]\n",
    "                    # delete id from delete_data\n",
    "                    for j in range(len(delete_data)):\n",
    "                        if delete_data[j][\"id\"] == id:\n",
    "                            delete_data.pop(j)\n",
    "                            break\n",
    "                    with open(delete_path, \"w\", encoding=\"utf-8\") as t:\n",
    "                        json.dump(delete_data, t, indent=2, ensure_ascii=False)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "        # for i in range(len(del_lst)):\n",
    "        #     data.pop(del_lst[i] - i)\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "with open(\"./tmp/v1.0-simplified-nq-dev-all.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]\n",
    "    print(len(data))\n",
    "    # dict: id, question\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": data[i][\"question_text\"],\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    with open(\"./data/NaturalQuestions.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    data = random.sample(data, 100)\n",
    "    data = sorted(data, key=lambda x: x[\"id\"])\n",
    "    \n",
    "    with open(\"./NaturalQuestions.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./tmp/LearningQ/data/experiments/teded/tgt-test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [line.strip() for line in f.readlines()]\n",
    "    data = [i for i in data if i.endswith(\"?\")]\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": data[i],\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    with open(\"./LearningQ.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = \"vicuna-13b\"\n",
    "files = [\n",
    "            \"BioASQ\",\n",
    "            \"nfcorpus\",\n",
    "            \"FiQA-2018\",\n",
    "            \"SciFact\",\n",
    "            \"LearningQ\",\n",
    "            \"NaturalQuestions\",\n",
    "            \"HotpotQA\",\n",
    "        ]\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    with open(f\"./{model}/{file}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        add = json.load(f)\n",
    "    with open(f\"./data/{file}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i][f\"{model}_response\"] = add[i][f\"{model}_response\"]\n",
    "\n",
    "    with open(f\"./data/{file}_.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5235\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/SciFact_add.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": data[i][\"user_query\"],\n",
    "            \"chatgpt_response\": data[i][\"chatgpt_response\"],\n",
    "            \"text-davinci-002_response\": data[i][\"text-davinci-002_response\"],\n",
    "            \"text-davinci-003_response\": data[i][\"text-davinci-003_response\"],\n",
    "            \"llama-2-7b-chat-hf_response\": data[i][\"llama-2-7b-chat-hf_response\"],\n",
    "            \"llama-2-13b-chat-hf_response\": data[i][\"llama-2-13b-chat-hf_response\"],\n",
    "            \"alpaca-7b_response\": data[i][\"alpaca-7b_response\"],\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    with open(\"./data/SciFact_add_.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read judge_part files\n",
    "import json\n",
    "import os\n",
    "\n",
    "for path in os.listdir(\"./judge_part\"):\n",
    "    with open(f\"./judge_part/{path}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        data = sorted(data, key=lambda x: x[\"id\"])\n",
    "        with open(f\"./judge_part/{path}_\", \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "for path in os.listdir(\"./data\"):\n",
    "    with open(f\"./data/{path}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        # for key end with \"response\", clear the multiple \"\\n\" of its value, and save it.\n",
    "        for i in range(len(data)):\n",
    "            for key in data[i].keys():\n",
    "                if key.endswith(\"response\"):\n",
    "                    ans = data[i][key]\n",
    "                    ans = ans.strip().split(\"\\n\")\n",
    "                    ans = \"\\n\".join([_ for _ in ans if _])\n",
    "                    data[i][key] = ans\n",
    "        with open(f\"./new/{path}\", \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"./data\"\n",
    "# files = [\n",
    "    # \"BioASQ\",\n",
    "    # \"nfcorpus\",\n",
    "    # \"FiQA-2018\",\n",
    "    # \"SciFact\",\n",
    "    # \"LearningQ\",\n",
    "    # \"NaturalQuestions\",\n",
    "    # \"HotpotQA\",\n",
    "# ]\n",
    "with open(f\"{path}/Open-Domain.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "with open(f\"{path}/halueval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    add = json.load(f)\n",
    "data.extend(add)\n",
    "for i in range(len(data)):\n",
    "    data[i][\"id\"] = i\n",
    "with open(f\"./data/Open-Domain.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "    json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./add/Open-Domain.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "data = sorted(data, key=lambda x: x[\"id\"])\n",
    "with open(\"./add/Open-Domain.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "    json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../hallu/data/hallu_add/qa.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f.readlines()][:20]\n",
    "    print(len(data))\n",
    "    # json file\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"user_query\": data[i][\"question\"].strip(),\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "with open(\"./Open-Domain_demo.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from response import check_exist\n",
    "\n",
    "data_path = \"./data/\"\n",
    "add_path = \"./chatgpt_greedy/\"\n",
    "save_path = \"./update_chatgpt_top-p/\"\n",
    "for path in os.listdir(data_path):\n",
    "    if path in [\n",
    "        # \"Bio-Medical.json\",\n",
    "        # \"Finance.json\",\n",
    "        # \"Science.json\",\n",
    "        # \"Education.json\",\n",
    "        # \"Open-Domain.json\",\n",
    "    ]:\n",
    "        continue\n",
    "    with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    with open(os.path.join(add_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        add = json.load(f)\n",
    "        ids = [i[\"id\"] for i in add]\n",
    "    print(len(set(ids)))    \n",
    "    # add = [i for i in data if i[\"id\"] not in ids]\n",
    "    # add = sorted(add, key=lambda x: x[\"id\"])\n",
    "    # check_exist(save_path)\n",
    "    # with open(os.path.join(save_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "    #     json.dump(add, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_path = \"./update_chatgpt_top-p/\"\n",
    "for path in os.listdir(data_path):\n",
    "    with open(os.path.join(data_path, path), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = sorted(data, key=lambda x: x[\"id\"])\n",
    "    with open(os.path.join(data_path, path), \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from response import check_exist\n",
    "\n",
    "file_lst = [\n",
    "    \"Bio-Medical\",\n",
    "    \"Finance\",\n",
    "    \"Science\",\n",
    "    \"Education\",\n",
    "    \"Open-Domain\",\n",
    "]\n",
    "path = \"./sft2/\"\n",
    "for sft_file in os.listdir(path):\n",
    "    if not sft_file.endswith(\".json\"):\n",
    "        continue\n",
    "    sft_type = sft_file.split(\"-\")[0]\n",
    "    save_path = os.path.join(path, sft_type)\n",
    "    check_exist(save_path)\n",
    "    with open(os.path.join(path, sft_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for data_file in file_lst:\n",
    "        temp = [i for i in data if i[\"dataset\"] == data_file]\n",
    "        temp = [\n",
    "            {\n",
    "                \"id\": i[\"id\"],\n",
    "                \"user_query\": i[\"user_query\"],\n",
    "                \"llama-7b_response\": i[\"output\"],\n",
    "            }\n",
    "            for i in temp\n",
    "        ]\n",
    "        temp = sorted(temp, key=lambda x: x[\"id\"])\n",
    "        with open(\n",
    "            os.path.join(save_path, data_file + \".json\"), \"w\", encoding=\"utf-8\"\n",
    "        ) as g:\n",
    "            json.dump(temp, g, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "CUR = 1\n",
    "# json line data\n",
    "data_dir = \"./prompt_improvement/human_detailed/chatgpt/\"\n",
    "if CUR:\n",
    "    model = \"chatgpt\"\n",
    "else:\n",
    "    model = \"llama-2-7b-chat-hf\"\n",
    "for path in os.listdir(data_dir):\n",
    "    data_path = os.path.join(data_dir, path)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if CUR:\n",
    "            data = [json.loads(line) for line in f.readlines()]\n",
    "        else:\n",
    "            data = json.load(f)\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": data[i][\"id\"],\n",
    "            \"user_query\": data[i][\"user_query\"].strip(),\n",
    "            model + \"_response\": data[i][model + \"_response\"].strip(),\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    data = sorted(data, key=lambda x: x[\"id\"])\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"./prompt_result/prompt_format/search_demo/chatgpt/\"\n",
    "query_dir = \"./prompt_result/prompt_format/generate_demo/chatgpt/\"\n",
    "for path in os.listdir(data_dir):\n",
    "    data_path = os.path.join(data_dir, path)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    query_path = os.path.join(query_dir, path)\n",
    "    with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        query = json.load(f)\n",
    "    ids = [i[\"id\"] for i in query]\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": ids[i],\n",
    "            \"user_query\": data[i][\"user_query\"].strip(),\n",
    "            \"chatgpt_response\": data[i][\"chatgpt_response\"].strip(),\n",
    "        }\n",
    "        for i in range(len(data))\n",
    "    ]\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update code\n",
    "import json\n",
    "import os\n",
    "\n",
    "def process_data_in_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if file_path.endswith('.json'):  # 只处理json文件\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = [json.loads(line) for line in f.readlines()]\n",
    "                data = [\n",
    "                    {\n",
    "                        \"id\": data[i][\"id\"],\n",
    "                        \"user_query\": data[i][\"user_query\"].strip(),\n",
    "                        \"claude-1_response\": data[i][\"response\"].strip(),\n",
    "                    }\n",
    "                    for i in range(len(data))\n",
    "                ]\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 遍历指定文件夹下的所有文件夹并处理数据\n",
    "data_folder = \"./prompt_result_copy/\"\n",
    "for folder in os.listdir(data_folder):\n",
    "    folder_path = os.path.join(data_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_data_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"./task/response/vicuna-13b/\"\n",
    "query_dir = \"./annotation/query/\"\n",
    "save_dir = \"./task/RLHF/rlhf_data/vicuna-13b/\"\n",
    "for path in os.listdir(data_dir):\n",
    "    data_path = os.path.join(data_dir, path)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    query_path = os.path.join(query_dir, path)\n",
    "    with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        query = json.load(f)\n",
    "    ids = [i[\"id\"] for i in query]\n",
    "    filtered_data = [i for i in data if i[\"id\"] not in ids]\n",
    "    filtered_data = sorted(filtered_data, key=lambda x: x[\"id\"])\n",
    "    save_path = os.path.join(save_dir, path)\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from response import check_exist\n",
    "\n",
    "data_dir = \"./response/\"\n",
    "add_dir = \"./add/\"\n",
    "for path in os.listdir(data_dir):\n",
    "    for file in os.listdir(os.path.join(data_dir, path)):\n",
    "        data_path = os.path.join(data_dir, path, file)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        key = [i for i in list(data[0].keys()) if i.endswith(\"response\")][0]\n",
    "        add = [i for i in data if not i[key]]\n",
    "        # delete key for elements in add list\n",
    "        for i in range(len(add)):\n",
    "            del add[i][key]\n",
    "        check_exist(os.path.join(add_dir, path))\n",
    "        add_path = os.path.join(add_dir, path, file)\n",
    "        with open(add_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(add, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from response import check_exist\n",
    "\n",
    "data_dir = \"./response/\"\n",
    "add_dir = \"./\"\n",
    "save_dir = \"./update_response/\"\n",
    "for path in os.listdir(add_dir):\n",
    "    for file in os.listdir(os.path.join(add_dir, path)):\n",
    "        add_path = os.path.join(add_dir, path, file)\n",
    "        with open(add_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            add = json.load(f)\n",
    "        data_path = os.path.join(data_dir, path, file)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        key = [i for i in list(data[0].keys()) if i.endswith(\"response\")][0]\n",
    "        data = [i for i in data if i[key]]\n",
    "        data.extend(add)\n",
    "        data = sorted(data, key=lambda x: x[\"id\"])\n",
    "        check_exist(os.path.join(save_dir, path))\n",
    "        save_path = os.path.join(save_dir, path, file)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from response import check_exist\n",
    "\n",
    "data_dir = \"./judge/\"\n",
    "add_dir = \"./all_data/add_judge/\"\n",
    "save_dir = \"./update_judge/\"\n",
    "for path in os.listdir(add_dir):\n",
    "    for file in os.listdir(os.path.join(add_dir, path)):\n",
    "        add_path = os.path.join(add_dir, path, file)\n",
    "        with open(add_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            add = json.load(f)\n",
    "        data_path = os.path.join(data_dir, path, file)\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        key = [i for i in list(data[0].keys()) if i.endswith(\"response\")][0]\n",
    "        data = [i for i in data if i[key]]\n",
    "        data.extend(add)\n",
    "        data = sorted(data, key=lambda x: x[\"id\"])\n",
    "        check_exist(os.path.join(save_dir, path))\n",
    "        save_path = os.path.join(save_dir, path, file)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import func_timeout\n",
    "from func_timeout import func_set_timeout\n",
    "\n",
    "def b():\n",
    "    while 1:\n",
    "        time.sleep(0.2)\n",
    "        print(222)\n",
    "\n",
    "@func_set_timeout(2.5)\n",
    "def a():\n",
    "    b()\n",
    "    while 1:\n",
    "        time.sleep(0.7)\n",
    "        print(111)\n",
    "\n",
    "try:\n",
    "    a()\n",
    "except func_timeout.exceptions.FunctionTimedOut:\n",
    "    print(\"timeout\")\n",
    "except Exception:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from response import check_exist\n",
    "\n",
    "file_lst = [\n",
    "    \"Bio-Medical\",\n",
    "    \"Finance\",\n",
    "    \"Science\",\n",
    "    \"Education\",\n",
    "    \"Open-Domain\",\n",
    "]\n",
    "path = \"./task/SFT/sft/\"\n",
    "for sft_file in os.listdir(path):\n",
    "    data_dir = os.path.join(path, sft_file)\n",
    "    for data_file in file_lst:\n",
    "        data_path = os.path.join(data_dir, data_file + \".json\")\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        data = [\n",
    "            {\n",
    "                \"id\": i[\"id\"],\n",
    "                \"user_query\": i[\"user_query\"].replace(\n",
    "                    \"You MUST give a detailed answer to the following question: \", \"\"\n",
    "                ),\n",
    "                \"llama-7b_response\": i[\"llama-7b_response\"],\n",
    "            }\n",
    "            for i in data\n",
    "        ]\n",
    "        data = sorted(data, key=lambda x: x[\"id\"])\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(data, g, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
